{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengfanxu/miniconda3/envs/fschat/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, CloverConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/data2/mengfanxu/huggingface/Qwen2.5-0.5B\", device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data2/mengfanxu/huggingface/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight cuda:7\n",
      "model.layers.0.self_attn.q_proj.weight cuda:7\n",
      "model.layers.0.self_attn.q_proj.bias cuda:7\n",
      "model.layers.0.self_attn.k_proj.weight cuda:7\n",
      "model.layers.0.self_attn.k_proj.bias cuda:7\n",
      "model.layers.0.self_attn.v_proj.weight cuda:7\n",
      "model.layers.0.self_attn.v_proj.bias cuda:7\n",
      "model.layers.0.self_attn.o_proj.weight cuda:7\n",
      "model.layers.0.mlp.gate_proj.weight cuda:7\n",
      "model.layers.0.mlp.up_proj.weight cuda:7\n",
      "model.layers.0.mlp.down_proj.weight cuda:7\n",
      "model.layers.0.input_layernorm.weight cuda:7\n",
      "model.layers.0.post_attention_layernorm.weight cuda:7\n",
      "model.layers.1.self_attn.q_proj.weight cuda:7\n",
      "model.layers.1.self_attn.q_proj.bias cuda:7\n",
      "model.layers.1.self_attn.k_proj.weight cuda:7\n",
      "model.layers.1.self_attn.k_proj.bias cuda:7\n",
      "model.layers.1.self_attn.v_proj.weight cuda:7\n",
      "model.layers.1.self_attn.v_proj.bias cuda:7\n",
      "model.layers.1.self_attn.o_proj.weight cuda:7\n",
      "model.layers.1.mlp.gate_proj.weight cuda:7\n",
      "model.layers.1.mlp.up_proj.weight cuda:7\n",
      "model.layers.1.mlp.down_proj.weight cuda:7\n",
      "model.layers.1.input_layernorm.weight cuda:7\n",
      "model.layers.1.post_attention_layernorm.weight cuda:7\n",
      "model.layers.2.self_attn.q_proj.weight cuda:7\n",
      "model.layers.2.self_attn.q_proj.bias cuda:7\n",
      "model.layers.2.self_attn.k_proj.weight cuda:7\n",
      "model.layers.2.self_attn.k_proj.bias cuda:7\n",
      "model.layers.2.self_attn.v_proj.weight cuda:7\n",
      "model.layers.2.self_attn.v_proj.bias cuda:7\n",
      "model.layers.2.self_attn.o_proj.weight cuda:7\n",
      "model.layers.2.mlp.gate_proj.weight cuda:7\n",
      "model.layers.2.mlp.up_proj.weight cuda:7\n",
      "model.layers.2.mlp.down_proj.weight cuda:7\n",
      "model.layers.2.input_layernorm.weight cuda:7\n",
      "model.layers.2.post_attention_layernorm.weight cuda:7\n",
      "model.layers.3.self_attn.q_proj.weight cuda:7\n",
      "model.layers.3.self_attn.q_proj.bias cuda:7\n",
      "model.layers.3.self_attn.k_proj.weight cuda:7\n",
      "model.layers.3.self_attn.k_proj.bias cuda:7\n",
      "model.layers.3.self_attn.v_proj.weight cuda:7\n",
      "model.layers.3.self_attn.v_proj.bias cuda:7\n",
      "model.layers.3.self_attn.o_proj.weight cuda:7\n",
      "model.layers.3.mlp.gate_proj.weight cuda:7\n",
      "model.layers.3.mlp.up_proj.weight cuda:7\n",
      "model.layers.3.mlp.down_proj.weight cuda:7\n",
      "model.layers.3.input_layernorm.weight cuda:7\n",
      "model.layers.3.post_attention_layernorm.weight cuda:7\n",
      "model.layers.4.self_attn.q_proj.weight cuda:7\n",
      "model.layers.4.self_attn.q_proj.bias cuda:7\n",
      "model.layers.4.self_attn.k_proj.weight cuda:7\n",
      "model.layers.4.self_attn.k_proj.bias cuda:7\n",
      "model.layers.4.self_attn.v_proj.weight cuda:7\n",
      "model.layers.4.self_attn.v_proj.bias cuda:7\n",
      "model.layers.4.self_attn.o_proj.weight cuda:7\n",
      "model.layers.4.mlp.gate_proj.weight cuda:7\n",
      "model.layers.4.mlp.up_proj.weight cuda:7\n",
      "model.layers.4.mlp.down_proj.weight cuda:7\n",
      "model.layers.4.input_layernorm.weight cuda:7\n",
      "model.layers.4.post_attention_layernorm.weight cuda:7\n",
      "model.layers.5.self_attn.q_proj.weight cuda:7\n",
      "model.layers.5.self_attn.q_proj.bias cuda:7\n",
      "model.layers.5.self_attn.k_proj.weight cuda:7\n",
      "model.layers.5.self_attn.k_proj.bias cuda:7\n",
      "model.layers.5.self_attn.v_proj.weight cuda:7\n",
      "model.layers.5.self_attn.v_proj.bias cuda:7\n",
      "model.layers.5.self_attn.o_proj.weight cuda:7\n",
      "model.layers.5.mlp.gate_proj.weight cuda:7\n",
      "model.layers.5.mlp.up_proj.weight cuda:7\n",
      "model.layers.5.mlp.down_proj.weight cuda:7\n",
      "model.layers.5.input_layernorm.weight cuda:7\n",
      "model.layers.5.post_attention_layernorm.weight cuda:7\n",
      "model.layers.6.self_attn.q_proj.weight cuda:7\n",
      "model.layers.6.self_attn.q_proj.bias cuda:7\n",
      "model.layers.6.self_attn.k_proj.weight cuda:7\n",
      "model.layers.6.self_attn.k_proj.bias cuda:7\n",
      "model.layers.6.self_attn.v_proj.weight cuda:7\n",
      "model.layers.6.self_attn.v_proj.bias cuda:7\n",
      "model.layers.6.self_attn.o_proj.weight cuda:7\n",
      "model.layers.6.mlp.gate_proj.weight cuda:7\n",
      "model.layers.6.mlp.up_proj.weight cuda:7\n",
      "model.layers.6.mlp.down_proj.weight cuda:7\n",
      "model.layers.6.input_layernorm.weight cuda:7\n",
      "model.layers.6.post_attention_layernorm.weight cuda:7\n",
      "model.layers.7.self_attn.q_proj.weight cuda:7\n",
      "model.layers.7.self_attn.q_proj.bias cuda:7\n",
      "model.layers.7.self_attn.k_proj.weight cuda:7\n",
      "model.layers.7.self_attn.k_proj.bias cuda:7\n",
      "model.layers.7.self_attn.v_proj.weight cuda:7\n",
      "model.layers.7.self_attn.v_proj.bias cuda:7\n",
      "model.layers.7.self_attn.o_proj.weight cuda:7\n",
      "model.layers.7.mlp.gate_proj.weight cuda:7\n",
      "model.layers.7.mlp.up_proj.weight cuda:7\n",
      "model.layers.7.mlp.down_proj.weight cuda:7\n",
      "model.layers.7.input_layernorm.weight cuda:7\n",
      "model.layers.7.post_attention_layernorm.weight cuda:7\n",
      "model.layers.8.self_attn.q_proj.weight cuda:7\n",
      "model.layers.8.self_attn.q_proj.bias cuda:7\n",
      "model.layers.8.self_attn.k_proj.weight cuda:7\n",
      "model.layers.8.self_attn.k_proj.bias cuda:7\n",
      "model.layers.8.self_attn.v_proj.weight cuda:7\n",
      "model.layers.8.self_attn.v_proj.bias cuda:7\n",
      "model.layers.8.self_attn.o_proj.weight cuda:7\n",
      "model.layers.8.mlp.gate_proj.weight cuda:7\n",
      "model.layers.8.mlp.up_proj.weight cuda:7\n",
      "model.layers.8.mlp.down_proj.weight cuda:7\n",
      "model.layers.8.input_layernorm.weight cuda:7\n",
      "model.layers.8.post_attention_layernorm.weight cuda:7\n",
      "model.layers.9.self_attn.q_proj.weight cuda:7\n",
      "model.layers.9.self_attn.q_proj.bias cuda:7\n",
      "model.layers.9.self_attn.k_proj.weight cuda:7\n",
      "model.layers.9.self_attn.k_proj.bias cuda:7\n",
      "model.layers.9.self_attn.v_proj.weight cuda:7\n",
      "model.layers.9.self_attn.v_proj.bias cuda:7\n",
      "model.layers.9.self_attn.o_proj.weight cuda:7\n",
      "model.layers.9.mlp.gate_proj.weight cuda:7\n",
      "model.layers.9.mlp.up_proj.weight cuda:7\n",
      "model.layers.9.mlp.down_proj.weight cuda:7\n",
      "model.layers.9.input_layernorm.weight cuda:7\n",
      "model.layers.9.post_attention_layernorm.weight cuda:7\n",
      "model.layers.10.self_attn.q_proj.weight cuda:7\n",
      "model.layers.10.self_attn.q_proj.bias cuda:7\n",
      "model.layers.10.self_attn.k_proj.weight cuda:7\n",
      "model.layers.10.self_attn.k_proj.bias cuda:7\n",
      "model.layers.10.self_attn.v_proj.weight cuda:7\n",
      "model.layers.10.self_attn.v_proj.bias cuda:7\n",
      "model.layers.10.self_attn.o_proj.weight cuda:7\n",
      "model.layers.10.mlp.gate_proj.weight cuda:7\n",
      "model.layers.10.mlp.up_proj.weight cuda:7\n",
      "model.layers.10.mlp.down_proj.weight cuda:7\n",
      "model.layers.10.input_layernorm.weight cuda:7\n",
      "model.layers.10.post_attention_layernorm.weight cuda:7\n",
      "model.layers.11.self_attn.q_proj.weight cuda:7\n",
      "model.layers.11.self_attn.q_proj.bias cuda:7\n",
      "model.layers.11.self_attn.k_proj.weight cuda:7\n",
      "model.layers.11.self_attn.k_proj.bias cuda:7\n",
      "model.layers.11.self_attn.v_proj.weight cuda:7\n",
      "model.layers.11.self_attn.v_proj.bias cuda:7\n",
      "model.layers.11.self_attn.o_proj.weight cuda:7\n",
      "model.layers.11.mlp.gate_proj.weight cuda:7\n",
      "model.layers.11.mlp.up_proj.weight cuda:7\n",
      "model.layers.11.mlp.down_proj.weight cuda:7\n",
      "model.layers.11.input_layernorm.weight cuda:7\n",
      "model.layers.11.post_attention_layernorm.weight cuda:7\n",
      "model.layers.12.self_attn.q_proj.weight cuda:7\n",
      "model.layers.12.self_attn.q_proj.bias cuda:7\n",
      "model.layers.12.self_attn.k_proj.weight cuda:7\n",
      "model.layers.12.self_attn.k_proj.bias cuda:7\n",
      "model.layers.12.self_attn.v_proj.weight cuda:7\n",
      "model.layers.12.self_attn.v_proj.bias cuda:7\n",
      "model.layers.12.self_attn.o_proj.weight cuda:7\n",
      "model.layers.12.mlp.gate_proj.weight cuda:7\n",
      "model.layers.12.mlp.up_proj.weight cuda:7\n",
      "model.layers.12.mlp.down_proj.weight cuda:7\n",
      "model.layers.12.input_layernorm.weight cuda:7\n",
      "model.layers.12.post_attention_layernorm.weight cuda:7\n",
      "model.layers.13.self_attn.q_proj.weight cuda:7\n",
      "model.layers.13.self_attn.q_proj.bias cuda:7\n",
      "model.layers.13.self_attn.k_proj.weight cuda:7\n",
      "model.layers.13.self_attn.k_proj.bias cuda:7\n",
      "model.layers.13.self_attn.v_proj.weight cuda:7\n",
      "model.layers.13.self_attn.v_proj.bias cuda:7\n",
      "model.layers.13.self_attn.o_proj.weight cuda:7\n",
      "model.layers.13.mlp.gate_proj.weight cuda:7\n",
      "model.layers.13.mlp.up_proj.weight cuda:7\n",
      "model.layers.13.mlp.down_proj.weight cuda:7\n",
      "model.layers.13.input_layernorm.weight cuda:7\n",
      "model.layers.13.post_attention_layernorm.weight cuda:7\n",
      "model.layers.14.self_attn.q_proj.weight cuda:7\n",
      "model.layers.14.self_attn.q_proj.bias cuda:7\n",
      "model.layers.14.self_attn.k_proj.weight cuda:7\n",
      "model.layers.14.self_attn.k_proj.bias cuda:7\n",
      "model.layers.14.self_attn.v_proj.weight cuda:7\n",
      "model.layers.14.self_attn.v_proj.bias cuda:7\n",
      "model.layers.14.self_attn.o_proj.weight cuda:7\n",
      "model.layers.14.mlp.gate_proj.weight cuda:7\n",
      "model.layers.14.mlp.up_proj.weight cuda:7\n",
      "model.layers.14.mlp.down_proj.weight cuda:7\n",
      "model.layers.14.input_layernorm.weight cuda:7\n",
      "model.layers.14.post_attention_layernorm.weight cuda:7\n",
      "model.layers.15.self_attn.q_proj.weight cuda:7\n",
      "model.layers.15.self_attn.q_proj.bias cuda:7\n",
      "model.layers.15.self_attn.k_proj.weight cuda:7\n",
      "model.layers.15.self_attn.k_proj.bias cuda:7\n",
      "model.layers.15.self_attn.v_proj.weight cuda:7\n",
      "model.layers.15.self_attn.v_proj.bias cuda:7\n",
      "model.layers.15.self_attn.o_proj.weight cuda:7\n",
      "model.layers.15.mlp.gate_proj.weight cuda:7\n",
      "model.layers.15.mlp.up_proj.weight cuda:7\n",
      "model.layers.15.mlp.down_proj.weight cuda:7\n",
      "model.layers.15.input_layernorm.weight cuda:7\n",
      "model.layers.15.post_attention_layernorm.weight cuda:7\n",
      "model.layers.16.self_attn.q_proj.weight cuda:7\n",
      "model.layers.16.self_attn.q_proj.bias cuda:7\n",
      "model.layers.16.self_attn.k_proj.weight cuda:7\n",
      "model.layers.16.self_attn.k_proj.bias cuda:7\n",
      "model.layers.16.self_attn.v_proj.weight cuda:7\n",
      "model.layers.16.self_attn.v_proj.bias cuda:7\n",
      "model.layers.16.self_attn.o_proj.weight cuda:7\n",
      "model.layers.16.mlp.gate_proj.weight cuda:7\n",
      "model.layers.16.mlp.up_proj.weight cuda:7\n",
      "model.layers.16.mlp.down_proj.weight cuda:7\n",
      "model.layers.16.input_layernorm.weight cuda:7\n",
      "model.layers.16.post_attention_layernorm.weight cuda:7\n",
      "model.layers.17.self_attn.q_proj.weight cuda:7\n",
      "model.layers.17.self_attn.q_proj.bias cuda:7\n",
      "model.layers.17.self_attn.k_proj.weight cuda:7\n",
      "model.layers.17.self_attn.k_proj.bias cuda:7\n",
      "model.layers.17.self_attn.v_proj.weight cuda:7\n",
      "model.layers.17.self_attn.v_proj.bias cuda:7\n",
      "model.layers.17.self_attn.o_proj.weight cuda:7\n",
      "model.layers.17.mlp.gate_proj.weight cuda:7\n",
      "model.layers.17.mlp.up_proj.weight cuda:7\n",
      "model.layers.17.mlp.down_proj.weight cuda:7\n",
      "model.layers.17.input_layernorm.weight cuda:7\n",
      "model.layers.17.post_attention_layernorm.weight cuda:7\n",
      "model.layers.18.self_attn.q_proj.weight cuda:7\n",
      "model.layers.18.self_attn.q_proj.bias cuda:7\n",
      "model.layers.18.self_attn.k_proj.weight cuda:7\n",
      "model.layers.18.self_attn.k_proj.bias cuda:7\n",
      "model.layers.18.self_attn.v_proj.weight cuda:7\n",
      "model.layers.18.self_attn.v_proj.bias cuda:7\n",
      "model.layers.18.self_attn.o_proj.weight cuda:7\n",
      "model.layers.18.mlp.gate_proj.weight cuda:7\n",
      "model.layers.18.mlp.up_proj.weight cuda:7\n",
      "model.layers.18.mlp.down_proj.weight cuda:7\n",
      "model.layers.18.input_layernorm.weight cuda:7\n",
      "model.layers.18.post_attention_layernorm.weight cuda:7\n",
      "model.layers.19.self_attn.q_proj.weight cuda:7\n",
      "model.layers.19.self_attn.q_proj.bias cuda:7\n",
      "model.layers.19.self_attn.k_proj.weight cuda:7\n",
      "model.layers.19.self_attn.k_proj.bias cuda:7\n",
      "model.layers.19.self_attn.v_proj.weight cuda:7\n",
      "model.layers.19.self_attn.v_proj.bias cuda:7\n",
      "model.layers.19.self_attn.o_proj.weight cuda:7\n",
      "model.layers.19.mlp.gate_proj.weight cuda:7\n",
      "model.layers.19.mlp.up_proj.weight cuda:7\n",
      "model.layers.19.mlp.down_proj.weight cuda:7\n",
      "model.layers.19.input_layernorm.weight cuda:7\n",
      "model.layers.19.post_attention_layernorm.weight cuda:7\n",
      "model.layers.20.self_attn.q_proj.weight cuda:7\n",
      "model.layers.20.self_attn.q_proj.bias cuda:7\n",
      "model.layers.20.self_attn.k_proj.weight cuda:7\n",
      "model.layers.20.self_attn.k_proj.bias cuda:7\n",
      "model.layers.20.self_attn.v_proj.weight cuda:7\n",
      "model.layers.20.self_attn.v_proj.bias cuda:7\n",
      "model.layers.20.self_attn.o_proj.weight cuda:7\n",
      "model.layers.20.mlp.gate_proj.weight cuda:7\n",
      "model.layers.20.mlp.up_proj.weight cuda:7\n",
      "model.layers.20.mlp.down_proj.weight cuda:7\n",
      "model.layers.20.input_layernorm.weight cuda:7\n",
      "model.layers.20.post_attention_layernorm.weight cuda:7\n",
      "model.layers.21.self_attn.q_proj.weight cuda:7\n",
      "model.layers.21.self_attn.q_proj.bias cuda:7\n",
      "model.layers.21.self_attn.k_proj.weight cuda:7\n",
      "model.layers.21.self_attn.k_proj.bias cuda:7\n",
      "model.layers.21.self_attn.v_proj.weight cuda:7\n",
      "model.layers.21.self_attn.v_proj.bias cuda:7\n",
      "model.layers.21.self_attn.o_proj.weight cuda:7\n",
      "model.layers.21.mlp.gate_proj.weight cuda:7\n",
      "model.layers.21.mlp.up_proj.weight cuda:7\n",
      "model.layers.21.mlp.down_proj.weight cuda:7\n",
      "model.layers.21.input_layernorm.weight cuda:7\n",
      "model.layers.21.post_attention_layernorm.weight cuda:7\n",
      "model.layers.22.self_attn.q_proj.weight cuda:7\n",
      "model.layers.22.self_attn.q_proj.bias cuda:7\n",
      "model.layers.22.self_attn.k_proj.weight cuda:7\n",
      "model.layers.22.self_attn.k_proj.bias cuda:7\n",
      "model.layers.22.self_attn.v_proj.weight cuda:7\n",
      "model.layers.22.self_attn.v_proj.bias cuda:7\n",
      "model.layers.22.self_attn.o_proj.weight cuda:7\n",
      "model.layers.22.mlp.gate_proj.weight cuda:7\n",
      "model.layers.22.mlp.up_proj.weight cuda:7\n",
      "model.layers.22.mlp.down_proj.weight cuda:7\n",
      "model.layers.22.input_layernorm.weight cuda:7\n",
      "model.layers.22.post_attention_layernorm.weight cuda:7\n",
      "model.layers.23.self_attn.q_proj.weight cuda:7\n",
      "model.layers.23.self_attn.q_proj.bias cuda:7\n",
      "model.layers.23.self_attn.k_proj.weight cuda:7\n",
      "model.layers.23.self_attn.k_proj.bias cuda:7\n",
      "model.layers.23.self_attn.v_proj.weight cuda:7\n",
      "model.layers.23.self_attn.v_proj.bias cuda:7\n",
      "model.layers.23.self_attn.o_proj.weight cuda:7\n",
      "model.layers.23.mlp.gate_proj.weight cuda:7\n",
      "model.layers.23.mlp.up_proj.weight cuda:7\n",
      "model.layers.23.mlp.down_proj.weight cuda:7\n",
      "model.layers.23.input_layernorm.weight cuda:7\n",
      "model.layers.23.post_attention_layernorm.weight cuda:7\n",
      "model.norm.weight cuda:7\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"/data2/mengfanxu/peft/src/peft/tuners/clover/clover_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tensor([[-0.0230, -0.0107,  0.0010,  ..., -0.0034,  0.0044,  0.0004],\n",
      "        [ 0.0149, -0.0086, -0.0119,  ..., -0.0032, -0.0049,  0.0055],\n",
      "        [-0.0004, -0.0016, -0.0137,  ..., -0.0011, -0.0012,  0.0080],\n",
      "        ...,\n",
      "        [-0.0033,  0.0299,  0.0026,  ..., -0.0395,  0.0252, -0.0089],\n",
      "        [ 0.0192, -0.0025,  0.0148,  ..., -0.0338, -0.0007, -0.0106],\n",
      "        [-0.0083,  0.0105,  0.0126,  ..., -0.0247,  0.0107,  0.0099]])\n",
      "R: tensor([[ 8.8886e+02, -8.6061e+00, -9.0715e+01,  ...,  7.3334e-01,\n",
      "         -1.3633e+01,  1.7544e+01],\n",
      "        [ 0.0000e+00, -8.7329e+02, -5.8500e+01,  ..., -4.8815e+01,\n",
      "         -1.9377e+01, -4.3866e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  8.6374e+02,  ..., -6.3226e+01,\n",
      "          8.3276e+01, -5.0930e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5508e-06,\n",
      "          9.7916e-07, -2.9510e-07],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          9.6434e-07, -7.7335e-07],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00, -5.4843e-07]])\n",
      "P (column permutation): tensor([3696, 2550,  724,  ..., 3905, 1050, 3061], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import qr\n",
    "\n",
    "# Example matrix\n",
    "A = (torch.randn(4096, 128)@torch.randn(128, 4096)).numpy()  # Convert PyTorch tensor to NumPy array\n",
    "# Pivoted QR decomposition using SciPy\n",
    "Q, R, P = qr(A, pivoting=True)\n",
    "\n",
    "# Convert results back to PyTorch tensors\n",
    "Q = torch.tensor(Q)\n",
    "R = torch.tensor(R)\n",
    "P = torch.tensor(P)\n",
    "\n",
    "print(\"Q:\", Q)\n",
    "print(\"R:\", R)\n",
    "print(\"P (column permutation):\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.8886e+02, -8.6061e+00, -9.0715e+01,  ...,  7.3334e-01,\n",
       "         -1.3633e+01,  1.7544e+01],\n",
       "        [ 0.0000e+00, -8.7329e+02, -5.8500e+01,  ..., -4.8815e+01,\n",
       "         -1.9377e+01, -4.3866e+01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  8.6374e+02,  ..., -6.3226e+01,\n",
       "          8.3276e+01, -5.0930e+01],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  9.8037e+00,\n",
       "          2.6055e+01,  6.7967e+01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.3755e+01,\n",
       "          1.4145e+02, -3.0345e+01],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.9294e+01,\n",
       "         -1.2916e+02,  2.8201e+01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight cuda:7\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.0.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.0.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.0.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.0.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.1.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.1.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.1.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.1.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.2.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.2.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.2.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.2.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.3.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.3.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.3.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.3.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.4.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.4.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.4.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.4.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.5.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.5.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.5.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.5.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.6.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.6.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.6.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.6.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.7.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.7.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.7.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.7.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.8.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.8.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.8.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.8.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.9.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.9.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.9.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.9.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.10.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.10.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.10.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.10.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.11.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.11.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.11.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.11.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.12.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.12.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.12.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.12.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.13.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.13.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.13.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.13.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.14.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.14.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.14.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.14.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.15.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.15.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.15.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.15.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.16.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.16.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.16.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.16.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.17.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.17.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.17.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.17.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.18.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.18.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.18.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.18.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.19.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.19.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.19.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.19.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.20.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.20.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.20.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.20.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.21.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.21.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.21.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.21.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.22.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.22.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.22.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.22.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.23.self_attn.q_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight cuda:7\n",
      "base_model.model.model.layers.23.self_attn.k_proj.bias cuda:7\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight cuda:7\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.bias cuda:7\n",
      "base_model.model.model.layers.23.self_attn.v_proj.clover_R.default cuda:7\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight cuda:7\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight cuda:7\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight cuda:7\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight cuda:7\n",
      "base_model.model.model.layers.23.input_layernorm.weight cuda:7\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight cuda:7\n",
      "base_model.model.model.norm.weight cuda:7\n"
     ]
    }
   ],
   "source": [
    "for name, param in peft_model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CloverConfig(init_clover_weights='qr', head_in_or_head_out={'q_proj':'in', 'v_proj':\"out\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.clover_R.default cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.3.self_attn.q_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.3.self_attn.v_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight cuda:1\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight cuda:1\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight cuda:1\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight cuda:1\n",
      "base_model.model.model.layers.3.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.4.self_attn.q_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.4.self_attn.v_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight cuda:1\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight cuda:1\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight cuda:1\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight cuda:1\n",
      "base_model.model.model.layers.4.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.5.self_attn.q_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.5.self_attn.v_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight cuda:1\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight cuda:1\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight cuda:1\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight cuda:1\n",
      "base_model.model.model.layers.5.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.6.self_attn.q_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.6.self_attn.v_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight cuda:1\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight cuda:1\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight cuda:1\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight cuda:1\n",
      "base_model.model.model.layers.6.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.7.self_attn.q_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.7.self_attn.v_proj.clover_R.default cuda:1\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight cuda:1\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight cuda:1\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight cuda:1\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight cuda:1\n",
      "base_model.model.model.layers.7.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.8.self_attn.q_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight cuda:2\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.8.self_attn.v_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight cuda:2\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight cuda:2\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight cuda:2\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight cuda:2\n",
      "base_model.model.model.layers.8.input_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.9.self_attn.q_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight cuda:2\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.9.self_attn.v_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight cuda:2\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight cuda:2\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight cuda:2\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight cuda:2\n",
      "base_model.model.model.layers.9.input_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.10.self_attn.q_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight cuda:2\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.10.self_attn.v_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight cuda:2\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight cuda:2\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight cuda:2\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight cuda:2\n",
      "base_model.model.model.layers.10.input_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.11.self_attn.q_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight cuda:2\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.11.self_attn.v_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight cuda:2\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight cuda:2\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight cuda:2\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight cuda:2\n",
      "base_model.model.model.layers.11.input_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.12.self_attn.q_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight cuda:2\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight cuda:2\n",
      "base_model.model.model.layers.12.self_attn.v_proj.clover_R.default cuda:2\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight cuda:2\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight cuda:2\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight cuda:2\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight cuda:2\n",
      "base_model.model.model.layers.12.input_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight cuda:2\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.13.self_attn.q_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight cuda:3\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.13.self_attn.v_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight cuda:3\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight cuda:3\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight cuda:3\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight cuda:3\n",
      "base_model.model.model.layers.13.input_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.14.self_attn.q_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight cuda:3\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.14.self_attn.v_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight cuda:3\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight cuda:3\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight cuda:3\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight cuda:3\n",
      "base_model.model.model.layers.14.input_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.15.self_attn.q_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight cuda:3\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.15.self_attn.v_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight cuda:3\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight cuda:3\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight cuda:3\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight cuda:3\n",
      "base_model.model.model.layers.15.input_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.16.self_attn.q_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight cuda:3\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.16.self_attn.v_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight cuda:3\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight cuda:3\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight cuda:3\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight cuda:3\n",
      "base_model.model.model.layers.16.input_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.17.self_attn.q_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight cuda:3\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight cuda:3\n",
      "base_model.model.model.layers.17.self_attn.v_proj.clover_R.default cuda:3\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight cuda:3\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight cuda:3\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight cuda:3\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight cuda:3\n",
      "base_model.model.model.layers.17.input_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight cuda:3\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.18.self_attn.q_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight cuda:4\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.18.self_attn.v_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight cuda:4\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight cuda:4\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight cuda:4\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight cuda:4\n",
      "base_model.model.model.layers.18.input_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.19.self_attn.q_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight cuda:4\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.19.self_attn.v_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight cuda:4\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight cuda:4\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight cuda:4\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight cuda:4\n",
      "base_model.model.model.layers.19.input_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.20.self_attn.q_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight cuda:4\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.20.self_attn.v_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight cuda:4\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight cuda:4\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight cuda:4\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight cuda:4\n",
      "base_model.model.model.layers.20.input_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.21.self_attn.q_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight cuda:4\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.21.self_attn.v_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight cuda:4\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight cuda:4\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight cuda:4\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight cuda:4\n",
      "base_model.model.model.layers.21.input_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.22.self_attn.q_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight cuda:4\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight cuda:4\n",
      "base_model.model.model.layers.22.self_attn.v_proj.clover_R.default cuda:4\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight cuda:4\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight cuda:4\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight cuda:4\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight cuda:4\n",
      "base_model.model.model.layers.22.input_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight cuda:4\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.23.self_attn.q_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight cuda:5\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.23.self_attn.v_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight cuda:5\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight cuda:5\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight cuda:5\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight cuda:5\n",
      "base_model.model.model.layers.23.input_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.24.self_attn.q_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight cuda:5\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.24.self_attn.v_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight cuda:5\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight cuda:5\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight cuda:5\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight cuda:5\n",
      "base_model.model.model.layers.24.input_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.25.self_attn.q_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight cuda:5\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.25.self_attn.v_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight cuda:5\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight cuda:5\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight cuda:5\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight cuda:5\n",
      "base_model.model.model.layers.25.input_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.26.self_attn.q_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight cuda:5\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.26.self_attn.v_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight cuda:5\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight cuda:5\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight cuda:5\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight cuda:5\n",
      "base_model.model.model.layers.26.input_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.27.self_attn.q_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight cuda:5\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight cuda:5\n",
      "base_model.model.model.layers.27.self_attn.v_proj.clover_R.default cuda:5\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight cuda:5\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight cuda:5\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight cuda:5\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight cuda:5\n",
      "base_model.model.model.layers.27.input_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight cuda:5\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.28.self_attn.q_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight cuda:6\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.28.self_attn.v_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight cuda:6\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight cuda:6\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight cuda:6\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight cuda:6\n",
      "base_model.model.model.layers.28.input_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.29.self_attn.q_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight cuda:6\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.29.self_attn.v_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight cuda:6\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight cuda:6\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight cuda:6\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight cuda:6\n",
      "base_model.model.model.layers.29.input_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.30.self_attn.q_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight cuda:6\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.30.self_attn.v_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight cuda:6\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight cuda:6\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight cuda:6\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight cuda:6\n",
      "base_model.model.model.layers.30.input_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.31.self_attn.q_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight cuda:6\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight cuda:6\n",
      "base_model.model.model.layers.31.self_attn.v_proj.clover_R.default cuda:6\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight cuda:6\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight cuda:6\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight cuda:6\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight cuda:6\n",
      "base_model.model.model.layers.31.input_layernorm.weight cuda:6\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight cuda:6\n",
      "base_model.model.model.norm.weight cuda:6\n",
      "base_model.model.lm_head.weight cuda:6\n"
     ]
    }
   ],
   "source": [
    "for name, param in peft_model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"clover_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "output = peft_model.generate(**tokenizer(\"write me a peom\",return_tensors='pt').to(\"cuda:7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"write me a peom about the power of love\\n\\nIn the heart of the night,\\nWhere love's light is bright,\\nIt shines like a beacon,\\nGuiding us through the night.\\n\\nLove is the compass,\\nGuiding us through the storm,\\nIt's the strength we need,\\nTo face the challenges we face.\\n\\nLove is the key,\\nTo unlock the secrets of the heart,\\nIt's the secret to our happiness,\\nAnd the joy that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the light that we need,\\nThe light that we need to see,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"write me a peom about the power of love\\n\\nIn the heart of the night,\\nWhere love's light is bright,\\nIt shines like a beacon,\\nGuiding us through the night.\\n\\nLove is the compass,\\nGuiding us through the storm,\\nIt's the strength we need,\\nTo face the challenges we face.\\n\\nLove is the key,\\nTo unlock the secrets of the heart,\\nIt's the secret to our happiness,\\nAnd the joy that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the light that we need,\\nThe light that we need to see,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we cherish,\\nIt's the love that we share,\\nAnd the love that we cherish.\\n\\nLove is the force that we need,\\nThe force that we need to overcome,\\nIt's the power that we need,\\nTo make our lives shine.\\n\\nLove is the key to our happiness,\\nThe key to our happiness,\\nIt's the secret to our success,\\nAnd the success that we find.\\n\\nLove is the source of life,\\nThe source of all that we are,\\nIt's the power that we need,\\nTo make our dreams come true.\\n\\nLove is the bond that we share,\\nThe bond that we\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = peft_model.generate(**tokenizer(\"write me a peom\",return_tensors='pt').to(\"cuda:7\"))\n",
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft.tuners.tuners_utils import BaseTunerLayer, check_adapters_to_merge\n",
    "\n",
    "class CloverLayer(BaseTunerLayer):\n",
    "    # All names of layers that may contain (trainable) adapter weights\n",
    "    adapter_layer_names = (\"clover_R\")\n",
    "\n",
    "    def __init__(self, base_layer: nn.Module, **kwargs) -> None:\n",
    "        self.base_layer = base_layer\n",
    "        self.num_head = {}\n",
    "        self.head_dim = {}\n",
    "        self.head_in = {}\n",
    "        self.clover_R = nn.ParameterDict({})\n",
    "        # Mark the weight as unmerged\n",
    "        self._disable_adapters = False\n",
    "        self.merged_adapters = []\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        base_layer = self.get_base_layer()\n",
    "        if isinstance(base_layer, nn.Linear):\n",
    "            in_features, out_features = base_layer.in_features, base_layer.out_features\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Unsupported layer type '{type(base_layer)}' encountered, proceed at your own risk.\", UserWarning\n",
    "            )\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def _check_forward_args(self, x, *args, **kwargs):\n",
    "        \"\"\"Check if the arguments are compatible with the configs and state of the model\"\"\"\n",
    "        adapter_names = kwargs.get(\"adapter_names\", None)\n",
    "        if adapter_names is None:\n",
    "            return\n",
    "\n",
    "        if len(x) != len(adapter_names):\n",
    "            msg = (\n",
    "                \"Length of `adapter_names` should be the same as the number of inputs, but got \"\n",
    "                f\"{len(adapter_names)} and {len(x)} respectively.\"\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        if self.merged:\n",
    "            # It is unclear what would be the right thing to do if users pass adapter_names and there are merged\n",
    "            # adapters. Therefore, it is better to raise an error in this case.\n",
    "            msg = \"Cannot pass `adapter_names` when there are merged adapters, please call `unmerge_adapter` first.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "class Linear(nn.Module, CloverLayer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_layer,\n",
    "        head_dim: int,\n",
    "        head_in: bool = False,\n",
    "        adapter_name: str = 'default',\n",
    "        init_clover_weights: str = 'eye', # Choices: ['eye','qr','absorb-decompose']\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        CloverLayer.__init__(self, base_layer, **kwargs)\n",
    "        self._active_adapter = adapter_name\n",
    "        self.update_layer(\n",
    "            adapter_name,\n",
    "            head_dim,\n",
    "            head_in,\n",
    "            init_clover_weights=init_clover_weights,\n",
    "        )\n",
    "\n",
    "    def update_layer(\n",
    "        self, adapter_name, head_dim, head_in, init_clover_weights\n",
    "    ):\n",
    "        self.head_dim[adapter_name] = head_dim\n",
    "        self.head_in[adapter_name] = head_in\n",
    "        # Actual trainable parameters\n",
    "        if head_in:\n",
    "            assert self.in_features % head_dim == 0\n",
    "            self.num_head[adapter_name] = self.in_features // head_dim\n",
    "        else:\n",
    "            assert self.out_features % head_dim == 0\n",
    "            self.num_head[adapter_name] = self.out_features // head_dim\n",
    "        weight_R = torch.randn((self.num_head[adapter_name], head_dim, head_dim))\n",
    "        self.clover_R[adapter_name] = nn.Parameter(weight_R)\n",
    "\n",
    "        # for inits that require access to the base weight, use gather_param_ctx so that the weight is gathered when using DeepSpeed\n",
    "        if init_clover_weights == \"qr\":\n",
    "            self.qr_decompose_init(adapter_name)\n",
    "        elif init_clover_weights == \"absorb-decompose\":\n",
    "            self.absorb_decompose_init(adapter_name, init_clover_weights)\n",
    "        else:\n",
    "            self.reset_clover_parameters(adapter_name)\n",
    "\n",
    "        #self.set_adapter(self.active_adapters)\n",
    "\n",
    "    def reset_clover_parameters(self, adapter_name):\n",
    "        if adapter_name in self.clover_R.keys():\n",
    "            weight_R = torch.eye(self.head_dim[adapter_name]).unsqueeze(0).repeat(self.num_head[adapter_name], 1, 1)\n",
    "            self.clover_R[adapter_name].data = weight_R\n",
    "\n",
    "    def qr_decompose_init(self, adapter_name):\n",
    "        dtype = self.base_layer.weight.dtype\n",
    "        base_weight = self.base_layer.weight.data # (out_dim, in_dim)\n",
    "        if self.head_in[adapter_name]:\n",
    "            base_weight = base_weight.view(-1, self.num_head[adapter_name], self.head_dim[adapter_name]) # (out_dim, num_heads, head_dim)\n",
    "            weight_R = []\n",
    "            for h in range(self.num_head[adapter_name]):\n",
    "                Q, R = torch.linalg.qr(base_weight[:,h].to(torch.float32)) # Q(out_dim, head_dim), R(head_dim, head_dim)\n",
    "                base_weight[:,h] = Q # (out_dim, head_dim)\n",
    "                weight_R.append(R.T)\n",
    "                \n",
    "            self.clover_R[adapter_name].data = torch.stack(weight_R).to(dtype) # (num_heads, head_dim, head_dim)\n",
    "            self.base_layer.weight.data = base_weight.reshape(-1, self.num_head[adapter_name]*self.head_dim[adapter_name]).contiguous()\n",
    "            \n",
    "        else:        \n",
    "            if self.base_layer.bias is not None:\n",
    "                base_bias = self.base_layer.bias.data.unsqueeze(1) # (out_dim, 1)\n",
    "                base_weight = torch.cat([base_weight, base_bias],dim=1)  # (out_dim, in_dim+1)\n",
    "                \n",
    "            base_weight = base_weight.view(self.num_head[adapter_name], self.head_dim[adapter_name], -1) # (num_heads, head_dim, in_dim) or (num_heads, head_dim, in_dim+1)\n",
    "            weight_R = []\n",
    "            for h in range(self.num_head[adapter_name]):\n",
    "                Q, R = torch.linalg.qr(base_weight[h].T.to(torch.float32)) # Q(in_dim, head_dim), R(head_dim, head_dim)\n",
    "                base_weight[h] = Q.T # (head_dim, in_dim) or (head_dim, in_dim+1)\n",
    "                weight_R.append(R)\n",
    "                \n",
    "            self.clover_R[adapter_name].data = torch.stack(weight_R).to(dtype) # (num_heads, head_dim, head_dim)\n",
    "            if self.base_layer.bias is not None:\n",
    "                self.base_layer.bias.data = base_weight[:,:,-1].reshape(-1).contiguous().to(dtype) # (out_dim, )\n",
    "                base_weight = base_weight[:,:,:-1].to(dtype) # (num_heads, head_dim, in_dim)\n",
    "\n",
    "            self.base_layer.weight.data = base_weight.reshape(self.num_head[adapter_name]*self.head_dim[adapter_name], -1).contiguous()\n",
    "\n",
    "    def absorb_decompose_init(adapter_name):\n",
    "        pass\n",
    "    \n",
    "    def merge(self, safe_merge: bool = True, adapter_names: Optional[list[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Merge the active adapter weights into the base weights\n",
    "\n",
    "        Args:\n",
    "            safe_merge (`bool`, *optional*):\n",
    "                If True, the merge operation will be performed in a copy of the original weights and check for NaNs\n",
    "                before merging the weights. This is useful if you want to check if the merge operation will produce\n",
    "                NaNs. Defaults to `True`.\n",
    "            adapter_names (`list[str]`, *optional*):\n",
    "                The list of adapter names that should be merged. If None, all active adapters will be merged. Defaults\n",
    "                to `None`.\n",
    "        \"\"\"\n",
    "        adapter_names = check_adapters_to_merge(self, adapter_names)\n",
    "        if not adapter_names:\n",
    "            # no adapter to merge\n",
    "            return\n",
    "\n",
    "        for active_adapter in adapter_names:\n",
    "            if active_adapter in self.clover_R.keys():\n",
    "                base_layer = self.get_base_layer()\n",
    "                # Note that safe_merge will be slower than the normal merge\n",
    "                # because of the copy operation.\n",
    "                base_weights = base_layer.weight.data.clone() # (out_dim, in_dim)\n",
    "                weight_R = self.clover_R[active_adapter].data #(num_head, head_dim, head_dim)\n",
    "                if self.head_in[active_adapter]:\n",
    "                    base_weights = base_weights.view(self.out_features, self.num_head[active_adapter], self.head_dim[active_adapter])\n",
    "                    base_weights = torch.einsum(\"ohd,hed->ohe\", base_weights, weight_R) \n",
    "                else:\n",
    "                    if base_layer.bias is not None:\n",
    "                        base_bias = base_layer.bias.data.clone().unsqueeze(1) # (out_dim, 1)\n",
    "                        base_weights = torch.cat([base_weights, base_bias], dim=1)\n",
    "                        \n",
    "                    base_weights = base_weights.view(self.num_head[active_adapter], self.head_dim[active_adapter], -1)\n",
    "                    base_weights = torch.einsum(\"hdi,hde->hei\", base_weights, weight_R)\n",
    "                base_weights = base_weights.reshape(self.out_features, -1).contiguous()\n",
    "                if not torch.isfinite(base_weights).all():\n",
    "                    raise ValueError(\n",
    "                        f\"NaNs detected in the merged weights. The adapter {active_adapter} seems to be broken\"\n",
    "                    )\n",
    "                if base_layer.bias is not None and not self.head_in[active_adapter]:\n",
    "                    base_layer.bias.data = base_weights[:,-1].contiguous()\n",
    "                    base_weights = base_weights[:,:-1].contiguous()\n",
    "                base_layer.weight.data = base_weights\n",
    "\n",
    "                self.merged_adapters.append(active_adapter)\n",
    "\n",
    "    def rotation(self, result, clover_R, num_head, head_dim):\n",
    "        bsz, seq, _ = result.shape\n",
    "        result = result.view(bsz, seq, num_head, head_dim)\n",
    "        result = torch.einsum(\"bshd,hde->bshe\", result, clover_R)\n",
    "        result = result.reshape(bsz, seq, num_head*head_dim).contiguous() \n",
    "        return result\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:\n",
    "        self._check_forward_args(x, *args, **kwargs)\n",
    "        adapter_names = kwargs.pop(\"adapter_names\", None)\n",
    "\n",
    "        if self.disable_adapters:\n",
    "            if self.merged:\n",
    "                self.unmerge()\n",
    "            result = self.base_layer(x, *args, **kwargs)\n",
    "        elif adapter_names is not None:\n",
    "            result = self._mixed_batch_forward(x, *args, adapter_names=adapter_names, **kwargs)\n",
    "        elif self.merged:\n",
    "            result = self.base_layer(x, *args, **kwargs)\n",
    "        else:\n",
    "            torch_x_dtype = x.dtype\n",
    "            for active_adapter in self.active_adapters:\n",
    "                if active_adapter not in self.clover_R.keys():\n",
    "                    continue\n",
    "                if not self.head_in[active_adapter]:\n",
    "                    continue\n",
    "                clover_R = self.clover_R[active_adapter]\n",
    "                x = self.rotation(x, clover_R, self.num_head[active_adapter], self.head_dim[active_adapter])\n",
    "            x = x.to(torch_x_dtype)\n",
    "                \n",
    "            result = self.base_layer(x, *args, **kwargs) # (bsz, seq, num_heads*head_dim)\n",
    "            torch_result_dtype = result.dtype\n",
    "            for active_adapter in self.active_adapters:\n",
    "                if active_adapter not in self.clover_R.keys():\n",
    "                    continue\n",
    "                if self.head_in[active_adapter]:\n",
    "                    continue\n",
    "                clover_R = self.clover_R[active_adapter]\n",
    "                result = self.rotation(result, clover_R, self.num_head[active_adapter], self.head_dim[active_adapter])\n",
    "            result = result.to(torch_result_dtype)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _mixed_batch_forward(\n",
    "        self, x: torch.Tensor, *args: Any, adapter_names: list[str], **kwargs: Any\n",
    "    ) -> torch.Tensor:\n",
    "        # This is a special method that handles the case when users pass the argument `adapter_names`. This is an\n",
    "        # extra argument that allows mixing different adapters in the same batch at inference time.\n",
    "\n",
    "        unique_adapters = set(adapter_names)\n",
    "        sub_batch_indices_list = []\n",
    "        for adapter in unique_adapters:\n",
    "            sub_batch_indices_list.append([index for index, item in enumerate(adapter_names) if item == adapter])\n",
    "            \n",
    "        for i, active_adapter in enumerate(unique_adapters):\n",
    "            if active_adapter == \"__base__\":\n",
    "                continue\n",
    "            if active_adapter not in self.clover_R.keys():\n",
    "                continue\n",
    "            if not self.head_in[active_adapter]:\n",
    "                    continue\n",
    "            clover_R = self.clover_R[active_adapter]\n",
    "            torch_x_dtype = x.dtype\n",
    "\n",
    "            # getting the sub-batch, passing it to CLOVER layers and updating the corresponding indices of the linear\n",
    "            # layer output\n",
    "            sub_batch = x[sub_batch_indices_list[i]].to(clover_R.dtype)\n",
    "            clover_x = self.rotation(sub_batch, clover_R, self.num_head[active_adapter], self.head_dim[active_adapter])\n",
    "            x[sub_batch_indices_list[i]] = clover_x.to(torch_x_dtype)\n",
    "            \n",
    "        result = self.base_layer(x, *args, **kwargs)\n",
    "        torch_result_dtype = result.dtype\n",
    "\n",
    "        for i, active_adapter in enumerate(unique_adapters):\n",
    "            if active_adapter == \"__base__\":\n",
    "                continue\n",
    "            if active_adapter not in self.clover_R.keys():\n",
    "                continue\n",
    "            if self.head_in[active_adapter]:\n",
    "                    continue\n",
    "            clover_R = self.clover_R[active_adapter]\n",
    "\n",
    "            # getting the sub-batch, passing it to CLOVER layers and updating the corresponding indices of the linear\n",
    "            # layer output\n",
    "            sub_batch = result[sub_batch_indices_list[i]].to(clover_R.dtype)\n",
    "            if self.head_in[active_adapter]:\n",
    "                    continue\n",
    "            clover_output = self.rotation(sub_batch, clover_R, self.num_head[active_adapter], self.head_dim[active_adapter])\n",
    "            result[sub_batch_indices_list[i]] = clover_output.to(torch_result_dtype)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        rep = super().__repr__()\n",
    "        return \"clover.\" + rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1147,  0.0594, -0.4917,  ..., -0.1536,  0.3382,  0.2766],\n",
      "         [ 0.3614,  1.6465,  0.3168,  ...,  0.0198, -0.9441, -0.0502],\n",
      "         [-1.3687,  0.7455,  0.4477,  ...,  0.2797,  0.4861, -0.3303],\n",
      "         ...,\n",
      "         [ 1.0341, -0.2328,  1.1667,  ...,  0.4766,  0.5874, -0.8720],\n",
      "         [-0.6536,  0.2651,  0.1632,  ..., -0.0279, -0.5445,  0.4384],\n",
      "         [ 0.2715, -0.4977, -0.2946,  ..., -0.5098,  0.2562,  0.2518]],\n",
      "\n",
      "        [[ 0.7479, -0.8701, -0.6882,  ...,  1.0735, -0.9788,  0.2787],\n",
      "         [ 1.1530, -0.2348,  0.5584,  ...,  1.3575, -0.4017, -0.2344],\n",
      "         [-0.1461,  0.4578, -0.9949,  ...,  0.0065,  0.1757, -0.1041],\n",
      "         ...,\n",
      "         [-0.2544, -1.1965, -0.5829,  ..., -0.7570,  0.4917, -0.5717],\n",
      "         [ 0.0058, -0.3130, -0.1992,  ..., -0.1002,  0.2319, -0.1983],\n",
      "         [ 0.4276,  0.4038, -0.2927,  ...,  0.0962,  0.0613, -0.3319]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "q_proj = nn.Linear(4096, 4096, bias=False)\n",
    "x = torch.randn(2,15,4096)\n",
    "print(q_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1147,  0.0594, -0.4917,  ..., -0.1536,  0.3382,  0.2766],\n",
      "         [ 0.3614,  1.6465,  0.3168,  ...,  0.0198, -0.9441, -0.0502],\n",
      "         [-1.3687,  0.7455,  0.4477,  ...,  0.2797,  0.4861, -0.3303],\n",
      "         ...,\n",
      "         [ 1.0341, -0.2328,  1.1667,  ...,  0.4766,  0.5874, -0.8720],\n",
      "         [-0.6536,  0.2651,  0.1632,  ..., -0.0279, -0.5445,  0.4384],\n",
      "         [ 0.2715, -0.4977, -0.2946,  ..., -0.5098,  0.2562,  0.2518]],\n",
      "\n",
      "        [[ 0.7479, -0.8701, -0.6882,  ...,  1.0735, -0.9788,  0.2787],\n",
      "         [ 1.1530, -0.2348,  0.5584,  ...,  1.3575, -0.4017, -0.2344],\n",
      "         [-0.1461,  0.4578, -0.9949,  ...,  0.0065,  0.1757, -0.1041],\n",
      "         ...,\n",
      "         [-0.2544, -1.1965, -0.5829,  ..., -0.7570,  0.4917, -0.5717],\n",
      "         [ 0.0058, -0.3130, -0.1992,  ..., -0.1002,  0.2319, -0.1983],\n",
      "         [ 0.4276,  0.4038, -0.2927,  ...,  0.0962,  0.0613, -0.3319]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clover_proj = Linear(q_proj, 128, head_in=True,init_clover_weights=\"qr\")\n",
    "print(clover_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clover_proj.merge(safe_merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1147,  0.0594, -0.4917,  ..., -0.1536,  0.3382,  0.2766],\n",
      "         [ 0.3614,  1.6465,  0.3168,  ...,  0.0198, -0.9441, -0.0502],\n",
      "         [-1.3687,  0.7455,  0.4477,  ...,  0.2797,  0.4861, -0.3303],\n",
      "         ...,\n",
      "         [ 1.0341, -0.2328,  1.1667,  ...,  0.4766,  0.5874, -0.8720],\n",
      "         [-0.6536,  0.2651,  0.1632,  ..., -0.0279, -0.5445,  0.4384],\n",
      "         [ 0.2715, -0.4977, -0.2946,  ..., -0.5098,  0.2562,  0.2518]],\n",
      "\n",
      "        [[ 0.7479, -0.8701, -0.6882,  ...,  1.0735, -0.9788,  0.2787],\n",
      "         [ 1.1530, -0.2348,  0.5584,  ...,  1.3575, -0.4017, -0.2344],\n",
      "         [-0.1461,  0.4578, -0.9949,  ...,  0.0065,  0.1757, -0.1041],\n",
      "         ...,\n",
      "         [-0.2544, -1.1965, -0.5829,  ..., -0.7570,  0.4917, -0.5717],\n",
      "         [ 0.0058, -0.3130, -0.1992,  ..., -0.1002,  0.2319, -0.1983],\n",
      "         [ 0.4276,  0.4038, -0.2927,  ...,  0.0962,  0.0613, -0.3319]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(clover_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5202e-02,  8.5682e-01,  2.9192e-01,  ..., -2.3793e-01,\n",
      "          -3.0827e-01,  2.2188e-01],\n",
      "         [-1.3650e-01,  6.0451e-01,  4.2241e-01,  ..., -2.6414e-01,\n",
      "          -1.3790e-01,  6.5217e-02],\n",
      "         [ 2.8061e-01,  4.0249e-01,  2.6483e-01,  ..., -3.4663e-01,\n",
      "          -1.3704e-01, -8.3752e-01],\n",
      "         ...,\n",
      "         [-4.2521e-01, -1.7031e-01,  3.1694e-01,  ..., -3.6772e-01,\n",
      "          -1.1709e-01,  2.5517e-01],\n",
      "         [ 5.9677e-01,  5.4387e-02,  1.6737e-01,  ..., -3.6323e-02,\n",
      "          -5.5437e-02, -5.0203e-03],\n",
      "         [ 6.0396e-02,  2.9254e-01, -1.5522e-01,  ..., -5.1261e-01,\n",
      "           2.5141e-01,  2.4361e-01]],\n",
      "\n",
      "        [[ 1.1792e-01, -3.6792e-01,  3.2642e-01,  ...,  2.0840e-01,\n",
      "           5.6022e-01,  6.9570e-02],\n",
      "         [ 1.7640e-01, -7.3480e-01, -3.9186e-01,  ...,  2.7923e-01,\n",
      "          -5.3349e-04,  6.8417e-01],\n",
      "         [ 2.9929e-01,  1.2432e-01,  3.4336e-01,  ..., -4.6908e-01,\n",
      "           4.1230e-01, -3.6135e-01],\n",
      "         ...,\n",
      "         [-1.7490e-02,  1.6587e-02,  1.7137e-01,  ...,  9.1232e-02,\n",
      "           4.0929e-03, -2.8948e-01],\n",
      "         [-1.9238e-01,  1.9381e-01,  1.6621e-01,  ..., -3.5150e-01,\n",
      "          -1.5741e-01, -3.7984e-01],\n",
      "         [ 3.4093e-01, -5.2004e-01,  4.4212e-01,  ..., -2.2389e-01,\n",
      "          -1.3667e-01,  1.0845e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(clover_proj._mixed_batch_forward(x,adapter_names=['default', 'default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clover.Linear(\n",
       "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (clover_R): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 32x128x128])\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clover_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fschat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
